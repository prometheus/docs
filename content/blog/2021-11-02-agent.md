---
title: Introducing Prometheus Agent Mode, an Efficient Cloud Native Way for Forwarding Metrics 
created_at: 2021-11-02
kind: article
author_name: Bartlomiej Plotka (@bwplotka)
---

> Bartek PÅ‚otka is a Prometheus Maintainer since 2019 and Principal Software Engineer at Red Hat. Co-author of the CNCF Thanos project. CNCF Ambassador and tech lead for the CNCF TAG Observability. In his free time he writes a book titled "Efficient Go" with O'Reilly. Opinions are my own!

What I personally love in Prometheus project and one of the many reasons why I joined the team, was the laser focus on the project goals. Prometheus was always about moving boundaries when it comes to providing pragmatic, reliable, cheap, yet invaluable metric monitoring. Its ultra stable and robust APIs, querying language and integration protocols (e.g. Remote Write and [Open Metrics](https://openmetrics.io/)) allowed the CNCF metric ecosystem to grow on those strong foundations. This is why we don't see Kubernetes clusters without Prometheus running there. This is also why we can see community exporters for getting metrics about literally everything e.g. [containers](https://github.com/google/cadvisor), [eBPF](https://github.com/cloudflare/ebpf_exporter), [Minecraft server statistics](https://github.com/sladkoff/minecraft-prometheus-exporter) and even [plants health when gardening](https://megamorf.gitlab.io/2019/07/14/monitoring-plant-health-with-prometheus/). 

The strong focus of the Prometheus community allowed other open source projects to grow (e.g. [Cortex](https://cortexmetrics.io/), [Thanos](https://thanos.io/) and more) to extend Prometheus data model beyond single clusters. Not mentioning cloud vendors adopting Prometheus API and data model (e.g. [Amazon Managed Prometheus](https://aws.amazon.com/prometheus/), [Google Cloud Managed Prometheus](https://cloud.google.com/stackdriver/docs/managed-prometheus), [Grafana Cloud](https://grafana.com/products/cloud/) and more). If you are looking for a single reason why Prometheus project is so successful, it is this: *Focusing the monitoring community on what matters*.

In this blog post I would love to introduce you a new mode of running Prometheus called "agent". It is built directly into the Prometheus binary. The agent mode blocks some of the Prometheus features and optimizes the binary for scraping and remote writing to remote location. While reducing number of features on the solution sounds like a regression, I will explain you why it is a game changer for certain deployments in the CNCF ecosystem. I am personally super excited for this! (:

## History of the Forwarding Use Case

The core design of Prometheus has been unchanged for the whole project lifetime. Inspired by the Google Borgmon, you can deploy it next to the application you want to monitor, tell Prometheus where they are and allow Prometheus to scrape the current values of their metrics every configured interval. Such collection method, which is often referred as "pull model" is the core principle that allows Prometheus to be lightweight and ultra reliable. Furthermore, it allows applications and exporters to be dead simple as they need to provide a simple human-readable HTTP page with current metric values (in Open Metric format), without complex push infrastructure as we typically see. Overall, in simple view, typical Prometheus monitoring deployment looks as presented below:

![Prometheus high level view](/assets/blog/2021-11-02/prom.png)

This works great, and we have seen millions successful deployments like this over the years that process millions of series. Some of them with longer time retention like 2 years so. All, allowing to query, alert and record metrics useful for both cluster admins, as well as developers.

However, the CNCF world constantly grows and evolves. With the grow of managed Kubernetes solutions and clusters created on demand within seconds we are now able to treat "clusters" as a "cattle" not "pet". In some cases solutions does not even have the cluster notion anymore e.g. [kcp](https://github.com/kcp-dev/kcp), [Google Cloud Run](https://cloud.google.com/run), [Fargate](https://aws.amazon.com/fargate/) and other serverless-like platforms.

Other interesting use case that emerges is the notion of **Edge** clusters or networks. With the industries like telecommunication, automotive and IoTs adopting cloud native technologies we see more and more smaller clusters with restricted amount of resources. This is forcing all observability data to be transferred to remote, bigger counterparts as almost nothing can be stored on those remote nodes.

What that means? That means monitoring data has to be somehow aggregated, presented to users and sometimes even stored on the *global* level. This is often called a **Global View** feature.

Naively we could think about implementing this by putting Prometheus on global level and scrape metrics cross networks, neither push metric directly from application to central location for monitoring purposes. Let me explain why those are bad ideas:

* Scrape across network is a terrible idea as we employ a huge amount of unknowns to our monitoring pipeline. The pull model allows Prometheus to know why exactly metric target has problems and when. Maybe it's down, misconfigured, restarted, too slow to give us metrics (e.g CPU saturated), not discoverable by service discovery, we don't have credentials to access or just DNS, network or whole cluster is down. By putting our scraper outside of the network we risk losing some of the information due to unreliable scrape and irregular scrape intervals. Don't do it, it's not worth it. (:
* Metric push directly from the application to some central location is equally bad. Especially when you monitor larger fleet you know literally nothing when you don't see metrics from remote application. Is application down? Is my receive pipeline down? Maybe application failed to authorize? Maybe it failed to get IP of my remote cluster? Maybe it's too slow? Maybe network is down? Such design is often a recipe for a failure (or at least shrug ðŸ¤·ðŸ½â€â™€ï¸ moments).

As discussed many times on various conferences, Prometheus introduced three main protocols to support the global view case. Each with its own pros and cons. Let's go briefly go through those. They are presented in orange colour on the diagram below.

![Prometheus global view](/assets/blog/2021-11-02/prom-remote.png)

* **Federation** was introduced as the first feature for aggregation purposes. It allows a special scrape done by Prometheus on global level for subset of metrics in the leaf Prometheus. Such "federation" scrape reduce some unknowns when done across networks, because federate metric endpoint has all metric timestamps. Yet it usually suffers with inability to federate all metrics and inability to not lose data during longer network partitions (minutes).
* **Remote Read** allows to directly select metrics from the Prometheus database without PromQL engine. You can deploy Prometheus or other solution (e.g Thanos) on global level to perform PromQL query there while fetching required metrics from multiple remote locations. This is really powerful as it allows you to store data "locally" and access it only when needed. Unfortunately there are cons too. Without feature like [Query Pushdown](https://github.com/thanos-io/thanos/issues/305) we are in extreme cases pulling GBs of compressed metric data to answer a single query. Also, if we have a network partition we are temporarily blind. Last but not least, certain security guidelines are not allowing ingress traffic, only egress one.
* Finally, we have **Remote Write** which seems to be the most popular choice nowadays. Since the agent mode focuses on remote write feature use cases, let's explain it in more details.

### Remote Write

The Prometheus Remote Write protocol allows us to forward (stream) all or subset of metrics collected by Prometheus to the remote location. You can configure Prometheus to forward some metrics (if you want, with all metadata and exemplars!) to one or more locations that supports Remote Write API. In fact Prometheus supports both ingesting as well as sending Remote Write!

While the official [Remote Write API specification is in review stage](https://docs.google.com/document/d/1LPhVRSFkGNSuU1fBd81ulhsCPR4hkSZyyBj1SZ8fWOM/edit) the ecosystem adopted Remote Write protocol as the default metric export protocol with many open source projects I mentioned before and Amazon, Google, Grafana, Logz.io and other cloud service.

Prometheus project also offers the official compliance tests for its APIs e.g. [remote-write sender compliance](https://github.com/prometheus/compliance/tree/main/remote_write_sender) for solution that offers Remote Write client capabilities.

Streaming data from such scraper allows to fullfil Global View and means that we store important information in separate to the application location. This also enables great separation of concerns, which is useful when applications are managed by different teams than the observability or monitoring pipelines. This is also why remote read is chosen by vendors who want to offload their customers from as much work as possible.

> But Bartek, you just mentioned above that pushing data directly from application is not the best idea!

Sure, but technically we still use pull model to gather metrics from applications, which gives us understanding of those different failure modes. Then we only export, replicate (push) data in batches to remote endpoint, which limits the number of monitoring unknowns central point has.

It's important to note that reliable remote-writing solution is a non-trivial problem to solve. Prometheus community spent around 3 years to come with stable and scalable implementation. We reimplemented WAL (Write-Ahead-Log), added internal queuing, sharding, smart back-offs and more. All of this is hidden from the user who can enjoy well-performing streaming or large amounts of metrics off clusters. 

### Hands-on Example: Katacoda

All of this is not new in Prometheus. Many already use Prometheus to scrape all required metrics and remote-write all or some of them to remote locations.

If you would like hands-on experience of remote write capabilities, we recommend our [Thanos Katacoda tutorial of remote writing metrics from Prometheus](https://katacoda.com/thanos/courses/thanos/3-receiver) which explains all steps required for Prometheus to forward all metrics to remote location.

So why we added special Agent mode to Prometheus?

## Prometheus Agent Mode

From Prometheus `v2.31.0` (next release), everyone will be able to run Prometheus binary in a new experimental `--enableagent` mode. If you want to try it before the release, feel free to build Prometheus from `main` branch or use our `quay.io/prometheus/prometheus:main` image.

The Agent mode optimizes Prometheus for remote write use case. It disables querying, alerting and local storage and replaces it with customized TSDB WAL. Everything else stays the same. Scraping logic, service discovery and related configuration. It can be used as a drop in replacement for Prometheus if you want to just forward your data to remote Prometheus and any other Remote Write compliant project. In the essence it looks like this:

![Prometheus agent](/assets/blog/2021-11-02/agent.png)

What is the benefit of using Agent mode if you plan to not query or alert on data locally and stream metrics outside? There are few:

First of all, efficiency. Our customized agent TSDB WAL removes the data immediately after successful writes. If it cannot reach the remote endpoint it persists the data temporarily on the disk until remote endpoint is back online. This is currently limited to 2h buffer only, similar to non-agent Prometheus, [hopefully unblocked soon](https://github.com/prometheus/prometheus/issues/9607). This means that we don't need to build chunks of data in memory. We don't need to maintain full index for querying purposes. Essentially Agent uses fraction of resources normal Prometheus would use in similar situation

Secondly, agent unlocks easier ingestion scalability. This is something I am excited the most. Prometheus in agent mode is more or less stateless. Yes, to avoid loss of metrics, HA replication and persistent disk has to be attached. But technically speaking if we have thousands of metric targets (e.g containers) we can deploy multiple Prometheus agents, and safely change which replica is scraping what targets. This essentially means easy, horizontal, auto-scaling capabilities of Prometheus scraping that can react to dynamic change in metric targets. This is definitely something we will look on with [Prometheus Kubernetes Operator](https://github.com/prometheus-operator/prometheus-operator) community.

The best part about Prometheus Agent is that it's built into Prometheus. Same scraping APIs, same semantics, same configuration and discovery mechanism.

Now let's take a look on currently implemented state of agent mode in Prometheus. Is it ready to use?

### Agent Mode Was Proven on Scale

Next release of Prometheus will include Agent mode as experimental feature. Flags, APIs and WAL format on disk might change. But the performance of implementation is already battle-tested thanks to [Grafana Labs](https://grafana.com/) open-source work.

Initial implementation of our agent, custom WAL was inspired by current Prometheus TSDB WAL and created by [Robert Fratto](https://github.com/rfratto) in 2019, under the mentorship of [Tom Wilkie](https://twitter.com/tom_wilkie), Prometheus maintainer. It was then used in open-source [Grafana Agent](https://github.com/grafana/agent) project that was since then, used by many Grafana Cloud customers and community members. Given maturity of the solution it was about the time to donate the implementation to Prometheus for native integration and bigger adoption. Robert (Grafana Labs) with the help of Srikrishna (Red Hat) and community ported the code to Prometheus codebase which was merged to `main` last week! ðŸ¤— 

It's important to understand that it's not easy to write amazing code 

The process was quite smooth. Since Prometheus maintainers saw the code before on Grafana Agent, plus new WAL is inspired by old one, it was not hard for current Prometheus TSDB maintainers to take it under full maintainance. 

TBD






## Summary

As always, if you have any issues or feedback, feel free to submit a ticket on GitHub or ask questions on the mailing list.
