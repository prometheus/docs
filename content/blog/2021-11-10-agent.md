---
title: Introducing Prometheus Agent Mode, an Efficient and Cloud Native Way for Metric Forwarding 
created_at: 2021-11-10
kind: article
author_name: Bartlomiej Plotka (@bwplotka)
---

> Bartek PÅ‚otka is a Prometheus Maintainer since 2019 and Principal Software Engineer at Red Hat. Co-author of the CNCF Thanos project. CNCF Ambassador and tech lead for the CNCF TAG Observability. In his free time he writes a book titled "Efficient Go" with O'Reilly. Opinions are my own!

What I personally love in Prometheus project and one of the many reasons why I joined the team, was the laser focus on the project goals. Prometheus was always about moving boundaries when it comes to providing pragmatic, reliable, cheap, yet invaluable metric monitoring. Prometheus ultra stable and robust APIs, querying language and integration protocols (e.g. Remote Write and [Open Metrics](https://openmetrics.io/)) allowed the Cloud Native Computing Foundation (CNCF) metric ecosystem to grow on those strong foundations. Amazing things happened as a result:

* We can see community exporters for getting metrics about literally everything e.g. [containers](https://github.com/google/cadvisor), [eBPF](https://github.com/cloudflare/ebpf_exporter), [Minecraft server statistics](https://github.com/sladkoff/minecraft-prometheus-exporter) and even [plants health when gardening](https://megamorf.gitlab.io/2019/07/14/monitoring-plant-health-with-prometheus/).
* Literally, every cloud-native software is expected to have an HTTP `/metric` endpoint that Prometheus can scrape.
* The observability paradigm shifted. We see SREs and developer rely heavily on metrics from day one, which improves software resiliency, debuggability and data-driven decision!

At the end, we hardly see Kubernetes clusters without Prometheus running there.

The strong focus of the Prometheus community allowed other open source projects to grow too to extend Prometheus data model beyond single clusters (e.g. [Cortex](https://cortexmetrics.io/), [Thanos](https://thanos.io/) and more). Not mentioning cloud vendors adopting Prometheus API and data model (e.g. [Amazon Managed Prometheus](https://aws.amazon.com/prometheus/), [Google Cloud Managed Prometheus](https://cloud.google.com/stackdriver/docs/managed-prometheus), [Grafana Cloud](https://grafana.com/products/cloud/) and more). If you are looking for a single reason why Prometheus project is so successful, it is this: **Focusing the monitoring community on what matters**.

In this (lengthy) blog post I would love to introduce you a new operational mode of running Prometheus called "agent". It is built directly into the Prometheus binary. The agent mode blocks some of the Prometheus features and optimizes the binary for scraping and remote writing to remote location. While reducing number of features on the solution sounds like a regression, I will explain you why it is a game-changer for certain deployments in the CNCF ecosystem. I am personally super excited for this! (:

## History of the Forwarding Use Case

The core design of Prometheus has been unchanged for the whole project lifetime. Inspired by the [Google Borgmon](https://sre.google/sre-book/practical-alerting/#the-rise-of-borgmon), you can deploy it next to the application you want to monitor, tell Prometheus where they are and allow Prometheus to scrape the current values of their metrics every configured interval. Such collection method, which is often referred as "pull model" is the core principle that allows Prometheus to be lightweight and ultra reliable. Furthermore, it allows applications and exporters to be dead simple as they need to provide a simple human-readable HTTP page with current metric values (in Open Metric format), without complex push infrastructure and non-trivial client libraries. Overall, in simple view, typical Prometheus monitoring deployment looks as presented below:

![Prometheus high level view](/assets/blog/2021-11-10/prom.png)

This works great, and we have seen millions successful deployments like this over the years that process dozens of millions of active series. Some of them for longer time retention like 2 years so. All, allowing to query, alert and record metrics useful for both cluster admins, as well as developers.

However, the CNCF world constantly grows and evolves. With the grow of managed Kubernetes solutions and clusters created on demand within seconds we are now finally able to treat "clusters" as a "cattle" not as "pet" (in other words we have more of those). In some cases solutions does not even have the cluster notion anymore e.g. [kcp](https://github.com/kcp-dev/kcp), [Google Cloud Run](https://cloud.google.com/run), [Fargate](https://aws.amazon.com/fargate/) and other serverless-like platforms.

![Yoda](/assets/blog/2021-11-10/yoda.gif)

The other interesting use case that emerges is the notion of **Edge** clusters or networks. With the industries like telecommunication, automotive and IoTs adopting cloud native technologies we see more and more much smaller clusters with restricted amount of resources. This is forcing all data (including observability) to be transferred to remote, bigger counterparts as almost nothing can be stored on those remote nodes.

What that means? That means monitoring data has to be somehow aggregated, presented to users and sometimes even stored on the *global* level. This is often called a **Global View** feature.

Naively we could think about implementing this by either putting Prometheus on that global level and scrape metrics across remote networks, or push metric directly from application to central location for monitoring purposes. Let me explain why both are generally a *very* bad ideas:

* Scrape across network is a terrible idea as we employ a huge amount of unknowns to our monitoring pipeline. The local pull model allows Prometheus to know why exactly metric target has problems and when. Maybe it's down, misconfigured, restarted, too slow to give us metrics (e.g CPU saturated), not discoverable by service discovery, we don't have credentials to access or just DNS, network or whole cluster is down. By putting our scraper outside of the network we risk losing some of the information due to unreliable scrape and irregular scrape intervals. On top of that we risk losing important visibility completely if the network is temporarily down. Don't do it, it's not worth it. (:
* Metric push directly from the application to some central location is equally bad. Especially when you monitor larger fleet you know literally nothing when you don't see metrics from remote application. Is application down? Is my receive pipeline down? Maybe application failed to authorize? Maybe it failed to get IP of my remote cluster? Maybe it's too slow? Maybe network is down? Such design is often a recipe for a failure (or at least shrug ðŸ¤·ðŸ½â€â™€ï¸ moments).

As discussed many times on various conferences, Prometheus introduced three main protocols to support the global view case. Each with its own pros and cons. Let's go briefly go through those. They are presented in orange colour on the diagram below.

![Prometheus global view](/assets/blog/2021-11-10/prom-remote.png)

* **Federation** was introduced as the first feature for aggregation purposes. It allows a special scrape done by Prometheus on global level for subset of metrics in the leaf Prometheus. Such "federation" scrape reduce some unknowns when done across networks, because metrics exposed by federate endpoint inject timestamps. Yet it usually suffers with inability to federate all metrics and inability to not lose data during longer network partitions (minutes).
* **Remote Read** allows to directly select metrics from the Prometheus database without PromQL engine. You can deploy Prometheus or other solution (e.g. Thanos) on the global level to perform PromQL query there while fetching required metrics from multiple remote locations. This is really powerful as it allows you to store data "locally" and access it only when needed. Unfortunately there are cons too. Without feature like [Query Pushdown](https://github.com/thanos-io/thanos/issues/305) we are in extreme cases pulling GBs of compressed metric data to answer a single query. Also, if we have a network partition we are temporarily blind. Last but not least, certain security guidelines are not allowing ingress traffic, only egress one.
* Finally, we have **Remote Write** which seems to be the most popular choice nowadays. Since the agent mode focuses on remote write feature use cases, let's explain it in more details.

### Remote Write

The Prometheus Remote Write protocol allows us to forward (stream) all or subset of metrics collected by Prometheus to the remote location. You can configure Prometheus to forward some metrics (if you want, with all metadata and exemplars!) to one or more locations that supports Remote Write API. In fact Prometheus supports both ingesting as well as sending Remote Write, so you can deploy Prometheus on global level to receive that stream and aggregate data cross-cluster.

While the official [Remote Write API specification is in review stage](https://docs.google.com/document/d/1LPhVRSFkGNSuU1fBd81ulhsCPR4hkSZyyBj1SZ8fWOM/edit) the ecosystem adopted Remote Write protocol as the default metric export protocol. For example, Cortex, Thanos, OpenTelemetry, plus cloud services like Amazon, Google, Grafana, Logz.io and others.

Prometheus project also offers the official compliance tests for its APIs e.g. [remote-write sender compliance](https://github.com/prometheus/compliance/tree/main/remote_write_sender) for solutions that offers Remote Write client capabilities. It's an amazing way to quickly tell if you are implementing this protocol in the correct way. 

Streaming data from such scraper allows to fullfill Global View and means that we can store important information in separate locations. This also enables great separation of concerns, which is useful when applications are managed by different teams than the observability or monitoring pipelines. Furthermore, it is also why Remote Write is chosen by vendors who want to offload their customers from as much work as possible.

> Wait a second Bartek. You just mentioned before that pushing metrics directly from application is not the best idea!

Sure, but the amazing part is that, even with the Remote Write, Prometheus still uses pull model to gather metrics from applications, which gives us understanding of those different failure modes. Only after that we batch samples and series and export, replicate (push) data to the Remote Write endpoints, which limit the number of monitoring unknowns central point has!

It's important to note that a reliable and efficient remote-writing implementation is a non-trivial problem to solve. The Prometheus community spent around 3 years to come with a stable and scalable implementation. We reimplemented WAL (Write-Ahead-Log) few times, added internal queuing, sharding, smart back-offs and more. All of this is hidden from the user who can enjoy well-performing streaming or large amounts of metrics off clusters.

### Hands-on Remote Write Example: Katacoda Tutorial

All of this is not new in Prometheus. Many of us already use Prometheus to scrape all required metrics and remote-write all or some of them to remote locations.

If you would like to try hands-on experience of remote write capabilities, we recommend our [Thanos Katacoda tutorial of remote writing metrics from Prometheus](https://katacoda.com/thanos/courses/thanos/3-receiver) which explains all steps required for Prometheus to forward all metrics to remote location. It's **free**, just sign up for an account and enjoy the tutorial! ðŸ¤—

Note, that this example uses Thanos in receive mode as the remote storage. Nowadays, you can use plenty of other project that are compatible with remote write API. 

So if remote writing works fine, why we added special Agent mode to Prometheus?

## Prometheus Agent Mode

From Prometheus `v2.32.0` (next release), everyone will be able to run Prometheus binary with an experimental `--enable-feature=agent` flag. If you want to try it before the release, feel free to use [Prometheus v2.32.0-beta.0](https://github.com/prometheus/prometheus/releases/tag/v2.32.0-beta.0) or use our `quay.io/prometheus/prometheus:v2.32.0-beta.0` image.

The Agent mode optimizes Prometheus for remote write use case. It disables querying, alerting and local storage and replaces it with customized TSDB WAL. Everything else stays the same. Scraping logic, service discovery and related configuration. It can be used as a drop in replacement for Prometheus if you want to just forward your data to remote Prometheus and any other Remote Write compliant project. In the essence it looks like this:

![Prometheus agent](/assets/blog/2021-11-10/agent.png)

The best part about Prometheus Agent is that it's built into Prometheus. Same scraping APIs, same semantics, same configuration and discovery mechanism.

What is the benefit of using Agent mode if you plan to not query or alert on data locally and stream metrics outside? There are few:

First of all, efficiency. Our customized agent TSDB WAL removes the data immediately after successful writes. If it cannot reach the remote endpoint it persists the data temporarily on the disk until remote endpoint is back online. This is currently limited to 2h buffer only, similar to non-agent Prometheus, [hopefully unblocked soon](https://github.com/prometheus/prometheus/issues/9607). This means that we don't need to build chunks of data in memory. We don't need to maintain full index for querying purposes. Essentially Agent uses fraction of resources normal Prometheus would use in similar situation. 

Does this efficiency matters? Yes! As we mentioned, for some deployments every GB of memory and every CPU core used on the edge clusters matters. On the other hand, the paradigm of performing monitoring using metric is quite mature these days. This means that the more relevant metrics with more cardinality you can ship for the same cost - the better.

> NOTE: With introduction of the Agent Mode, the original Prometheus server mode still stays as the recommended, stable and maintained mode. Agent Mode with remote storages brings additional complexity. Use with care. 

Secondly benefit of the new Agent mode, is that it enables easier ingestion horizontal scalability. This is something I am excited the most. Let me explain why.

### The Dream: Auto-Scalable Metric Ingestion

The true auto-scaling solution for scraping would be based on amount of metric target and amount of metrics they expose. The more data we have to scrape, the more instances of Prometheus we deploy automatically. If the number of targets or its data goes down, we could scale down and remove a couple of instances. This would remove the manual burden of adjusting the sizing of Prometheus and will stop the need of over-allocating Prometheus for situations where cluster is temporarily small.

With just Prometheus in server mode this was hard to achieve. This is because Prometheus in server mode is stateful. Whatever is collected stays as-is in single place. This means that scale down procedure would need to somehow back up the collected data to existing instances before termination. Then we would have problem of overlapped scraped, misleading staleness markers etc. On top of that we would need some global view query that is able to aggregate all samples across all instances (e.g Thanos Query or Promxy). Last, but not the least Prometheus in server mode resource usage depends on more things than just ingestion. There is alerting, recording, querying, compaction, remote write etc that might need more or less resources independently on number of metric targets.   

Agent Mode essentially moves the discovery, scraping and remote writing to a separate microservice. This allows focused operational model on ingestion only. As a result Prometheus in Agent mode is more or less stateless. Yes, to avoid loss of metrics, HA replication and persistent disk has to be attached. But technically speaking if we have thousands of metric targets (e.g containers) we can deploy multiple Prometheus agents, and safely change which replica is scraping what targets. This is because at the end all samples will be pushed the same central storage. 

Overall, Prometheus in Agent enables easy, horizontal, auto-scaling capabilities of Prometheus scraping that can react to dynamic change in metric targets. This is definitely something we will look on with [Prometheus Kubernetes Operator](https://github.com/prometheus-operator/prometheus-operator) community going forward.

Now let's take a look on currently implemented state of agent mode in Prometheus. Is it ready to use?

### Agent Mode Was Proven on Scale

The next release of Prometheus will include Agent mode as experimental feature. Flags, APIs and WAL format on disk might change. But the performance of implementation is already battle-tested thanks to [Grafana Labs](https://grafana.com/) open-source work.

Initial implementation of our agent, custom WAL was inspired by current Prometheus TSDB WAL and created by [Robert Fratto](https://github.com/rfratto) in 2019, under the mentorship of [Tom Wilkie](https://twitter.com/tom_wilkie), Prometheus maintainer. It was then used in open-source [Grafana Agent](https://github.com/grafana/agent) project that was since then, used by many Grafana Cloud customers and community members. Given maturity of the solution it was about the time to donate the implementation to Prometheus for native integration and bigger adoption. Robert (Grafana Labs) with the help of Srikrishna (Red Hat) and community ported the code to Prometheus codebase which was merged to `main` 2 weeks ago! ðŸ¤— 

The donation process was quite smooth. Since some Prometheus maintainers saw and contributed to this code before on Grafana Agent, plus new WAL is inspired by the Prometheus server one, it was not hard for current Prometheus TSDB maintainers to take it under full maintainance! It's especially easier, with Robert joining Prometheus Team as the TSDB maintainer (congratulations!).

Now, let's explain how you can use it! (:

### How to use Agent in details.

From now on, if print help of Prometheus you should see more or less following:

```bash
usage: prometheus [<flags>]

The Prometheus monitoring server

Flags:
  -h, --help                     Show context-sensitive help (also try --help-long and --help-man).
      (... other flags)
      --storage.tsdb.path="data/"
                                 Base path for metrics storage. Use with server mode only.
      --storage.agent.path="data-agent/"
                                 Base path for metrics storage. Use with agent mode only.
      (... other flags)
      --enable-feature= ...      Comma separated feature names to enable. Valid options: agent, exemplar-storage, expand-external-labels, memory-snapshot-on-shutdown, promql-at-modifier, promql-negative-offset, remote-write-receiver,
                                 extra-scrape-metrics, new-service-discovery-manager. See https://prometheus.io/docs/prometheus/latest/feature_flags/ for more details.
```

Since agent mode is behind feature flag, as mentioned previously, use `--enable-feature=agent` flag to run Prometheus in the Agent mode. Now, rest of flags are either for both server and agent, or only for specific mode. You can see which flag is for what mode by checking last sentence of flag help. `Use with server mode only` means it's only for server mode. If you don't see any mention like this, it means the flag is shared.

Agent accepts the same scrape configuration with the same discovery options and remote write options.

It also exposes UI, that has turned off query view, but still you can visit agent UI to check build info, configuration, targets and service discovery as in normal Prometheus. 

### Hands-on Prometheus Agent Example: Katacoda Tutorial

Similarly to Prometheus remote-write tutorial, if you would like to try hands-on experience of Prometheus Agent capabilities, we recommend our [Thanos Katacoda tutorial of Prometheus Agent](https://katacoda.com/thanos/courses/thanos/4-receiver-agent) which explains how easy is to run Prometheus Agent.

## Summary

I hope you found this interesting! In this post we walked through the new cases that emerged in CNCF, which motivated the project to enable new operational mode. We discussed what Agent mode gives and how to use it. 

As always, if you have any issues or feedback, feel free to submit a ticket on GitHub or ask questions on the mailing list.
