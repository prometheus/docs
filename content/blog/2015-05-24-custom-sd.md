---
title: Plugging in your Custom Service Discovery
created_at: 2015-05-24
kind: article
author: Fabian Reinartz
---

This week we released Prometheus v0.14 — a version with many long-awaited additions
and improvements.

On the user side, the the new means for service discovery aim to make it easier to
dynamically change the targets Prometheus scrapes. [Consul](https://www.consul.io) is
now supported out-of-the-box and a file-based plugin mechanism allows you to connect
your own discovery to Prometheus.

In this blog post, we will take a closer look at the latter option and provide a hands-on
example to push your changes from etcd into Prometheus instantaniously.


## How does it work?

Setting up your own service discovery involves three steps:

1. Extract your scrape targets from your service discovery
2. Transform them into groups of targets Prometheus can read
3. Write them to disk for Prometheus to read

We actually skipped step 0 — which is to tell Prometheus where those files will be
located.

A popular way to do service discovery are distributed key-value stores. As they put no
constraints on how to do service discovery, a custom bridge is an efficient approach to
overcome Prometheus's lack of knowledge about your setup.


## SD bridge by example

Let's get started with a fictional setup of an etcd cluster in which our services and
their instances are stored in the following format:

```
/services
    /service-x
        /info         = { "owner": "prodeng", "monitored": true }
        /instances
            /inst-1   = { "host": "10.33.1.230", "port": 5441, "group": "production" }
            ...
            /inst-n   = { "host": "10.33.1.244", "port": 8223, "group": "canary" }

    /service-y
        ...
```

Our keys contain JSON-serialized data. For the `*/info` keys this a simple map of labels
that provide metadata about the service.
Each instance is a JSON object containing the host and port of the instance, as well as
the a group it belongs to within the service.

Our goal is to make Prometheus scrape all services, that are flagged for monitoring and
assign the correct group and owner labels to all targets.


### Step 1 — Extract

First, we want to write a simple Go program that continiously watches etcd for changes in our 
`/services` directory.

We define data structures that map to our stored instances and service information:

```
// Instance is the instance object stored in etcd.
type Instance struct {
	Host  string `json:"host"`
	Port  int    `json:"port"`
	Group string `json:"group"`
}

// ServiceInfo is the information object stored in etcd.
type ServiceInfo struct {
	Owner     string `json:"owner"`
	Monitored bool   `json:"monitored"`
}
```

The current state of our service discovery is stored inside our program in the following structs.

```
// service is a service stored in etcd.
type service struct {
	info      *ServiceInfo
	instances map[string]*Instance // Instances by their path.
}

// services are the services stored in etcd.
type services struct {
	m   map[string]*service // The current services by name.
	del []string            // Services deleted in the last update.
}
```

Using the etcd client library we now perform one initial scan of the `/services` directory 
and populate our just defined structs. We then proceed by watching the whole directory
recursively.
Whenever we receive a setting or deleting operation on one of the nodes, we update our state.
Each time our state changes we persist it to disk in a Prometheus-readbable format.

This procedure results in the following `main()` function for our SD bridge:

```
func main() {
	flag.Parse()

	client := etcd.NewClient([]string{etcdServer})

	srvs := &services{
		m: map[string]*service{},
	}
	updates := make(chan *etcd.Response)

	// Perform an initial read of all services.
	res, err := client.Get(servicesPrefix, false, true)
	if err != nil {
		log.Fatalf("Error on initial retrieval: %s", err)
	}
	srvs.update(res.Node)
	srvs.persist()

	// Start watching for updates.
	go func() {
		res, err := client.Watch(servicesPrefix, 0, true, updates, nil)
		if err != nil {
			log.Errorln(err)
		}
		log.Infoln(res)
	}()

	// Apply updates sent on the channel.
	for res := range updates {
		if res.Action == "delete" {
			srvs.delete(res.Node)
		} else {
			srvs.update(res.Node)
		}
		srvs.persist()
	}
}
```

The `services.delete()` and `services.update()` methods are not listed here but can be found
in the [repository of this example](https://github.com/fabxc/prom_etcd_sd).

With this we now have a setup that successfully reads all our instances from our custom service
discovery setup. This leads us to...


### Step 2 — Transform

Prometheus can read files that contain lists of so called "target groups". A target group is a set
of addresses, which Prometheus should scrape, in combination with a set of labels that are assigned
to all metrics scraped from those targets.

A file containing a list of targets groups may be in JSON or YAML format and must have the respective
file extension.

In JSON format, a target group file looks as follows:

```
[
  {
    targets: ["10.33.1.204:8081", "10.32.100.20:12666"],
    labels: {
      "owner": "team-x",
      "group": "production"
    }
  },
  ...
]
```

The respective representation in Go looks as follows:

```
// TargetGroup is the target group read by Prometheus.
type TargetGroup struct {
	Targets []string          `json:"targets,omitempty"`
	Labels  map[string]string `json:"labels,omitempty"`
}
```

We now give our `service` type a method that provides is with a list of `TargetGroup`s for this service:

```
func (srv *service) targetGroups() (tgs []*TargetGroup) {
	groups := make(map[string][]*Instance)
	for _, inst := range srv.instances {
		groups[inst.Group] = append(groups[inst.Group], inst)
	}
	for grp, instances := range groups {
		tg := &TargetGroup{
			Labels: map[string]string{
				"group": grp,
				"owner": srv.info.Owner,
			},
		}
		for _, inst := range instances {
			tg.Targets = append(tg.Targets, fmt.Sprintf("%s:%d", inst.Host, inst.Port))
		}
		tgs = append(tgs, tg)
	}
	return tgs
}
```

### Step 3 — Persisting

All we have to do now, is to write those target groups to disk. This happens in the `services.persist()`
method which we call after applying each update to our internal structure (see the `main()` method above).

For each service we retrieve our target group representation, marshal it into JSON and write it into a file
named after our service.
Finally, we delete the files of services that are no long present or being monitored.

```
func (srvs *services) persist() {
	for name, srv := range srvs.m {
		if !srv.info.Monitored {
			continue
		}
		content, err := json.Marshal(srv.targetGroups())
		if err != nil {
			log.Errorln(err)
			continue
		}

		f, err := create("tgroups/" + name + ".json")
		if err != nil {
			log.Errorln(err)
			continue
		}
		if _, err := f.Write(content); err != nil {
			log.Errorln(err)
		}
		f.Close()
	}
	// Remove files for disappeared services.
	for _, name := range srvs.del {
		if err := os.Remove("tgroups/" + name + ".json"); err != nil {
			log.Errorln(err)
		}
	}
	srvs.del = nil
}
```

Prometheus does not apply changes to target group files that do not result in a valid list of 
target group. Nonetheless, it is recommended to first write the new file to disk and and rename it to its
eventual name afterwards (which is an atomic operation on POSIX systems).

In our example this procedure is hidden behind our `create()` method with which we open a file.


### Connecting to Prometheus

Starting up our little bridge program will now sync all changes to disc in a format Prometheus understands.
We now tell Prometheus where to look for the files via the `file_sd_configs` key in a scrape configuration.

For a small amount of services or if you are generating your configuration files, having one scrape 
configurations per service is a viable option. In this case we specify

```
scrape_configs:
  - job_name: 'service-x'
  
    file_sd_configs:
      - names: ['tgroups/service-x.json']

  - job_name: 'service-y'

    file_sd_configs:
      - names: ['tgroups/service-y.json']

  ...
```

For a more flexible setup we can leverage the concept of relabeling.
Relabeling is the process of taking a set of labels, applying a regex to their values, and writing a value
into another label based on it.

As our example bridge only writes target groups for services that are marked for monitoring, we can dynamically
scrape all targets of all those services. This starts and stops scraping services as they appear or disappear
as files of target groups.
Whenever a target is read from a file, a temporary `__meta_filepath` label is attached to it. Its value is the
path of the file it was extracted from.

```
scrape_configs:
  # In this scenario, the defined job_name will always be overwritten.
  - job_name: 'default'

  	file_sd_configs:
  	  - names: ['tgroups/*.json']

    relabel_configs:
      # Here we define a relabeling step that sets the 'job' label
      # to the name of the service.
      - source_labels: ['__meta_filepath']
        regex:         '^tgroups/(.+)\.json$'
        target_label:  'job'
        replacement:   '$1'
        action:        replace
```
