---
title: Introducing Prometheus Agent Mode, an Efficient and Cloud-Native Way for Metric Forwarding
created_at: 2021-11-10
kind: article
author_name: Bartlomiej Plotka (@bwplotka)
---

> Bartek PÅ‚otka has been a Prometheus Maintainer since 2019 and Principal Software Engineer at Red Hat. Co-author of the CNCF Thanos project. CNCF Ambassador and tech lead for the CNCF TAG Observability. In his free time, he writes a book titled "Efficient Go" with O'Reilly. Opinions are my own!

What I personally love in the Prometheus project, and one of the many reasons why I joined the team, was the laser focus on the project goals. Prometheus was always about moving boundaries when it comes to providing pragmatic, reliable, cheap, yet invaluable metric monitoring. Prometheus ultra-stable and robust APIs, querying language and integration protocols (e.g. Remote Write and [Open Metrics](https://openmetrics.io/)) allowed the Cloud Native Computing Foundation (CNCF) metric ecosystem to grow on those strong foundations. Amazing things happened as a result:

* We can see community exporters for getting metrics about literally everything e.g. [containers](https://github.com/google/cadvisor), [eBPF](https://github.com/cloudflare/ebpf_exporter), [Minecraft server statistics](https://github.com/sladkoff/minecraft-prometheus-exporter) and even [plants health when gardening](https://megamorf.gitlab.io/2019/07/14/monitoring-plant-health-with-prometheus/).
* Literally, every cloud-native software is expected to have an HTTP `/metric` endpoint that Prometheus can scrape.
* The observability paradigm shifted. We see SREs and developers rely heavily on metrics from day one, which improves software resiliency, debuggability and data-driven decision!

In the end, we hardly see Kubernetes clusters without Prometheus running there.

The strong focus of the Prometheus community allowed other open-source projects to grow too to extend the Prometheus data model beyond single clusters (e.g. [Cortex](https://cortexmetrics.io/), [Thanos](https://thanos.io/) and more). Not mentioning cloud vendors adopting Prometheus API and data model (e.g. [Amazon Managed Prometheus](https://aws.amazon.com/prometheus/), [Google Cloud Managed Prometheus](https://cloud.google.com/stackdriver/docs/managed-prometheus), [Grafana Cloud](https://grafana.com/products/cloud/) and more). If you are looking for a single reason why the Prometheus project is so successful, it is this: **Focusing the monitoring community on what matters**.

In this (lengthy) blog post, I would love to introduce a new operational mode of running Prometheus called "Agent". It is built directly into the Prometheus binary. The agent mode blocks some of the Prometheus features and optimizes the binary for scraping and remote writing to remote locations. While reducing the number of features on the solution sounds like a regression, I will explain why it is a game-changer for certain deployments in the CNCF ecosystem. I am super excited about this! (:

## History of the Forwarding Use Case

The core design of Prometheus has been unchanged for the whole project lifetime. Inspired by the [Google Borgmon](https://sre.google/sre-book/practical-alerting/#the-rise-of-borgmon), you can deploy it next to the application you want to monitor, tell Prometheus where they are and allow Prometheus to scrape the current values of their metrics every configured interval. Such a collection method, which is often referred to as the "pull model", is the core principle that allows Prometheus to be lightweight and ultra-reliable. Furthermore, it will enable applications and exporters to be dead simple as they need to provide a simple human-readable HTTP page with current metric values (in Open Metric format), without complex push infrastructure and non-trivial client libraries. Overall, in simple view, typical Prometheus monitoring deployment looks as presented below:

![Prometheus high-level view](/assets/blog/2021-11-10/prom.png)

This works great, and we have seen millions of successful deployments like this over the years that process dozens of millions of active series. Some of them for longer time retention, like two years so. All allow to query, alert, and record metrics useful for both cluster admins and developers.

However, the CNCF world constantly grows and evolves. With the growth of managed Kubernetes solutions and clusters created on-demand within seconds, we are now finally able to treat "clusters" as "cattle", not as "pets" (in other words, we have more of those, how many cats you can have in your home? ðŸ™ƒ ). In some cases, solutions do not even have the cluster notion anymore, e.g. [kcp](https://github.com/kcp-dev/kcp), [Google Cloud Run](https://cloud.google.com/run), [Fargate](https://aws.amazon.com/fargate/) and other serverless-like platforms.

![Yoda](/assets/blog/2021-11-10/yoda.gif)

The other interesting use case that emerges is the notion of **Edge** clusters or networks. With the industries like telecommunication, automotive and IoT devices adopting cloud-native technologies, we see more and more much smaller clusters with a restricted amount of resources. This is forcing all data (including observability) to be transferred to remote, bigger counterparts as almost nothing can be stored on those remote nodes.

What does that mean? That means monitoring data has to be somehow aggregated, presented to users and sometimes even stored on the *global* level. This is often called a **Global-View** feature.

Naively, we could think about implementing this by either putting Prometheus on that global level and scraping metrics across remote networks or pushing metrics directly from the application to the central location for monitoring purposes. Let me explain why both are generally *very* bad ideas:

* Scrape across the network is a terrible idea as we employ a huge amount of unknowns in our monitoring pipeline. The local pull model allows Prometheus to know why exactly the metric target has problems and when. Maybe it's down, misconfigured, restarted, too slow to give us metrics (e.g. CPU saturated), not discoverable by service discovery, we don't have credentials to access or just DNS, network, or whole cluster is down. By putting our scraper outside of the network, we risk losing some of the information due to unreliable scrape and irregular scrape intervals. On top of that, we risk losing important visibility completely if the network is temporarily down. Please don't do it. It's not worth it. (:
* Metric push directly from the application to some central location is equally bad. Especially when you monitor a larger fleet, you know nothing literally when you don't see metrics from remote applications. Is the application down? Is my receive pipeline down? Maybe the application failed to authorize? Maybe it failed to get the IP address of my remote cluster? Maybe it's too slow? Maybe the network is down? Such design is often a recipe for a failure (or at least shrug ðŸ¤·ðŸ½â€â™€ï¸ moments).

Prometheus introduced three main protocols to support the global view case, as discussed many times at various conferences. Each with its own pros and cons. Let's go briefly go through those. They are presented in orange colour on the diagram below.

![Prometheus global view](/assets/blog/2021-11-10/prom-remote.png)

* **Federation** was introduced as the first feature for aggregation purposes. It allows a special scrape done by Prometheus on a global level for a subset of metrics in the leaf Prometheus. Such a "federation" scrape reduces some unknowns across networks because metrics exposed by federate endpoints inject timestamps. Yet, it usually suffers from the inability to federate all metrics and not lose data during longer network partitions (minutes).
* **Remote Read** allows to select metrics from the Prometheus database without the PromQL engine directly. You can deploy Prometheus or other solutions (e.g. Thanos) on the global level to perform PromQL queries there while fetching required metrics from multiple remote locations. This is really powerful as it allows you to store data "locally" and access it only when needed. Unfortunately, there are cons too. Without features like [Query Pushdown](https://github.com/thanos-io/thanos/issues/305) we are in extreme cases pulling GBs of compressed metric data to answer a single query. Also, if we have a network partition, we are temporarily blind. Last but not least, certain security guidelines are not allowing ingress traffic, only egress one.
* Finally, we have **Remote Write**, which seems to be the most popular choice nowadays. Since the agent mode focuses on remote write feature use cases, let's explain it in more detail.

### Remote Write

The Prometheus Remote Write protocol allows us to forward (stream) all or subset of metrics collected by Prometheus to the remote location. You can configure Prometheus to forward some metrics (if you want, with all metadata and exemplars!) to one or more locations that support Remote Write API. In fact, Prometheus supports both ingesting and sending Remote Write, so you can deploy Prometheus on a global level to receive that stream and aggregate data cross-cluster.

While the official [Remote Write API specification is in review stage](https://docs.google.com/document/d/1LPhVRSFkGNSuU1fBd81ulhsCPR4hkSZyyBj1SZ8fWOM/edit), the ecosystem adopted the Remote Write protocol as the default metric export protocol. For example, Cortex, Thanos, OpenTelemetry, and cloud services like Amazon, Google, Grafana, Logz.io, etc.

Prometheus project also offers the official compliance tests for its APIs, e.g. [remote-write sender compliance](https://github.com/prometheus/compliance/tree/main/remote_write_sender) for solutions that offer Remote Write client capabilities. It's an amazing way to quickly tell if you are correctly implementing this protocol.

Streaming data from such scraper allows to fulfil Global View and store important information in separate locations. This also enables great separation of concerns, which is useful when applications are managed by different teams than the observability or monitoring pipelines. Furthermore, it is also why Remote Write is chosen by vendors who want to offload their customers from as much work as possible.

> Wait for a second, Bartek. You just mentioned before that pushing metrics directly from the application is not the best idea!

Sure, but the amazing part is that, even with the Remote Write, Prometheus still uses a pull model to gather metrics from applications, which gives us an understanding of those different failure modes. After that, we batch samples and series and export, replicate (push) data to the Remote Write endpoints, limiting the number of monitoring unknowns central point has!

It's important to note that a reliable and efficient remote-writing implementation is a non-trivial problem to solve. The Prometheus community spent around three years to come with a stable and scalable implementation. We reimplemented WAL (Write-Ahead-Log) a few times, added internal queuing, sharding, smart back-offs and more. All of this is hidden from the user who can enjoy well-performing streaming or large amounts of metrics off clusters.

### Hands-on Remote Write Example: Katacoda Tutorial

All of this is not new in Prometheus. Many of us already use Prometheus to scrape all required metrics and remote-write all or some of them to remote locations.

Suppose you would like to try the hands-on experience of remote writing capabilities. In that case, we recommend our [Thanos Katacoda tutorial of remote writing metrics from Prometheus](https://katacoda.com/thanos/courses/thanos/3-receiver), which explains all steps required for Prometheus to forward all metrics to the remote location. It's **free**, just sign up for an account and enjoy the tutorial! ðŸ¤—

Note that this example uses Thanos in receive mode as the remote storage. Nowadays, you can use plenty of other projects that are compatible with remote write API.

So if remote writing works fine, why do we add special Agent mode to Prometheus?

## Prometheus Agent Mode

From Prometheus `v2.32.0` (next release), everyone will be able to run Prometheus binary with an experimental `--enable-feature=agent` flag. If you want to try it before the release, feel free to use [Prometheus v2.32.0-beta.0](https://github.com/prometheus/prometheus/releases/tag/v2.32.0-beta.0) or use our `quay.io/prometheus/prometheus:v2.32.0-beta.0` image.

The Agent mode optimizes Prometheus for the remote write use case. It disables querying, alerting and local storage and replaces it with customized TSDB WAL. Everything else stays the same: scraping logic, service discovery and related configuration. It can be used as a drop-in replacement for Prometheus if you want to just forward your data to remote Prometheus and any other Remote Write compliant project. In essence it looks like this:

![Prometheus agent](/assets/blog/2021-11-10/agent.png)

The best part about Prometheus Agent is that it's built into Prometheus. Same scraping APIs, same semantics, same configuration and discovery mechanism.

What benefits of using Agent mode if you plan not to query or alert on data locally and stream metrics outside? There are a few:

First of all, efficiency. Our customized Agent TSDB WAL removes the data immediately after successful writes. If it cannot reach the remote endpoint, it persists the data temporarily on the disk until the remote endpoint is back online. This is currently limited to 2h buffer only, similar to non-agent Prometheus, [hopefully unblocked soon](https://github.com/prometheus/prometheus/issues/9607). This means that we don't need to build chunks of data in memory. We don't need to maintain a full index for querying purposes. Essentially Agent uses a fraction of resources normal Prometheus would use in a similar situation.

Does this efficiency matter? Yes! As we mentioned, every GB of memory and every CPU core used on the edge clusters matters for some deployments. On the other hand, the paradigm of performing monitoring using metrics is quite mature these days. This means that the more relevant metrics with more cardinality you can ship for the same cost - the better.

> NOTE: With the introduction of the Agent Mode, the original Prometheus server mode still stays as the recommended, stable and maintained mode. Agent Mode with remote storage brings additional complexity. Use with care.

Secondly, the benefit of the new Agent mode is that it enables easier ingestion horizontal scalability. This is something I am excited about the most. Let me explain why.

### The Dream: Auto-Scalable Metric Ingestion

The true auto-scaling solution for scraping would be based on the amount of metric target and the number of metrics they expose. The more data we have to scrape, the more instances of Prometheus we deploy automatically. If the number of targets or its data goes down, we could scale down and remove a couple of instances. This would remove the manual burden of adjusting the sizing of Prometheus and stop the need to over-allocating Prometheus for situations where the cluster is temporarily small.

With just Prometheus in server mode, this was hard to achieve. This is because Prometheus in server mode is stateful. Whatever is collected stays as-is in a single place. This means that the scale-down procedure would need to back up the collected data to existing instances before termination. Then we would have the problem of overlapped scraped, misleading staleness markers etc.

On top of that, we would need some global view query that is able to aggregate all samples across all instances (e.g. Thanos Query or Promxy). Last but not least, Prometheus in server mode resource usage depends on more things than just ingestion. There is alerting, recording, querying, compaction, remote write etc., that might need more or fewer resources independently on the number of metric targets.

Agent Mode essentially moves the discovery, scraping and remote writing to a separate microservice. This allows a focused operational model on ingestion only. As a result, Prometheus in Agent mode is more or less stateless. Yes, to avoid loss of metrics, HA replication and the persistent disk has to be attached. But technically speaking, if we have thousands of metric targets (e.g. containers), we can deploy multiple Prometheus agents and safely change which replica is scraping what targets. This is because, in the end, all samples will be pushed to the same central storage.

Overall, Prometheus in Agent enables easy, horizontal, auto-scaling capabilities of Prometheus scraping that can react to dynamic change in metric targets. This is definitely something we will look on with the [Prometheus Kubernetes Operator](https://github.com/prometheus-operator/prometheus-operator) community going forward.

Now let's take a look at the currently implemented state of agent mode in Prometheus. Is it ready to use?

### Agent Mode Was Proven on Scale

The next release of Prometheus will include Agent mode as an experimental feature. Flags, APIs and WAL format on disk might change. But the performance of the implementation is already battle-tested thanks to [Grafana Labs](https://grafana.com/) open-source work.

Initial implementation of our Agent, custom WAL was inspired by current Prometheus TSDB WAL and created by [Robert Fratto](https://github.com/rfratto) in 2019, under the mentorship of [Tom Wilkie](https://twitter.com/tom_wilkie), Prometheus maintainer. It was then used in an open-source [Grafana Agent](https://github.com/grafana/agent) project that was since then, used by many Grafana Cloud customers and community members. Given the maturity of the solution, it was time to donate the implementation to Prometheus for native integration and bigger adoption. Robert (Grafana Labs), with the help of Srikrishna (Red Hat) and the community, ported the code to Prometheus codebase, which was merged to `main` 2 weeks ago! ðŸ¤—

The donation process was quite smooth. Since some Prometheus maintainers saw and contributed to this code before on Grafana Agent, plus new WAL is inspired by the Prometheus server one, it was not hard for current Prometheus TSDB maintainers to take it under full maintenance! It's especially easier with Robert joining Prometheus Team as the TSDB maintainer (congratulations!).

Now, let's explain how you can use it! (:

### How to use Agent in details

From now on, if print help of Prometheus, you should see more or less the following:

```bash
usage: prometheus [<flags>]

The Prometheus monitoring server

Flags:
  -h, --help                     Show context-sensitive help (also try --help-long and --help-man).
      (... other flags)
      --storage.tsdb.path="data/"
                                 Base path for metrics storage. Use with server mode only.
      --storage.agent.path="data-agent/"
                                 Base path for metrics storage. Use with agent mode only.
      (... other flags)
      --enable-feature= ...      Comma separated feature names to enable. Valid options: agent, exemplar-storage, expand-external-labels, memory-snapshot-on-shutdown, promql-at-modifier, promql-negative-offset, remote-write-receiver,
                                 extra-scrape-metrics, new-service-discovery-manager. See https://prometheus.io/docs/prometheus/latest/feature_flags/ for more details.
```

Since agent mode is behind a feature flag, as mentioned previously, use `--enable-feature=agent` flag to run Prometheus in the Agent mode. Now, the rest of the flags are either for both server and Agent or only for a specific mode. You can see which flag is for what mode by checking the last sentence of flag help. `Use with server mode only` means it's only for server mode. If you don't see any mention like this, it means the flag is shared.

Agent accepts the same scrape configuration with the same discovery options and remote write options.

It also exposes UI that has turned off query view, but still, you can visit agent UI to check build info, configuration, targets and service discovery as in normal Prometheus.

### Hands-on Prometheus Agent Example: Katacoda Tutorial

Similarly to Prometheus remote-write tutorial, if you would like to try the hands-on experience of Prometheus Agent capabilities, we recommend our [Thanos Katacoda tutorial of Prometheus Agent](https://katacoda.com/thanos/courses/thanos/4-receiver-agent), which explains how easy it is to run Prometheus Agent.

## Summary

I hope you found this interesting! In this post, we walked through the new cases that emerged in CNCF, which motivated the project to enable a new operational mode. We discussed what Agent mode gives and how to use it.

As always, if you have any issues or feedback, feel free to submit a ticket on GitHub or ask questions on the mailing list.
